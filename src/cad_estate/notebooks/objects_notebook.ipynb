{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2023 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "#\n",
    "# Author: spopov@google.com (Stefan Popov)\n",
    "\n",
    "WORKDIR = '/path/to/cad_estate/'\n",
    "\n",
    "import asyncio\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.join(WORKDIR, 'src'))\n",
    "\n",
    "import ipywidgets\n",
    "import nest_asyncio\n",
    "import torch as t\n",
    "import torchvision.transforms.functional as tvtF\n",
    "\n",
    "from cad_estate import debug_helpers\n",
    "from cad_estate import file_system as fs\n",
    "from cad_estate import frames as frame_lib\n",
    "from cad_estate import objects as obj_lib\n",
    "from cad_estate.notebooks import objects_notebook_helper as helper_lib\n",
    "\n",
    "debug_helpers.better_jupyter_display()\n",
    "debug_helpers.better_tensor_display()\n",
    "nest_asyncio.apply()\n",
    "disp = debug_helpers.display_images\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Set to the full path of the annotations for scene you would like to visualize\n",
    "ANNOTATIONS_DIR = os.path.join(WORKDIR, \"data/annotations\")\n",
    "\n",
    "EXAMPLE_SCENES = [\n",
    "    \"-4qd6olV30Y_57424033\", \"-4czxXKNrnI_179946000\", \"5LToC4KInNM_84017000\",\n",
    "    \"5RD3EAlBS9w_180880000\", \"5TJZHqkmTlo_86921000\", \"-4czxXKNrnI_199665000\",\n",
    "    \"-23esP--xK8_30497133\", \"-AplGqzOF5Y_147046900\", \"5RD3EAlBS9w_112012000\"\n",
    "]  # yapf: ignore\n",
    "\n",
    "# If set to None, frames will be downloaded on the fly\n",
    "# FRAMES_DIR = None\n",
    "FRAMES_DIR = os.path.join(WORKDIR, \"data/frames\")\n",
    "\n",
    "# Path to the processed ShapeNet dataset\n",
    "SHAPE_NET_DIR = os.path.join(WORKDIR, \"data/shape_net_npz\")\n",
    "\n",
    "# How many frames to show (ignored when also showing tracks for\n",
    "# objects without 3D)\n",
    "NUM_FRAMES = 6\n",
    "\n",
    "# The width of the displayed images\n",
    "IMAGE_WIDTH = 800\n",
    "\n",
    "# One-time setup\n",
    "ANNOTATIONS_DIR = fs.abspath(ANNOTATIONS_DIR)\n",
    "SHAPE_NET_DIR = fs.abspath(SHAPE_NET_DIR)\n",
    "shapenet_meta = obj_lib.load_shapenet_metadata(SHAPE_NET_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive widgets setup\n",
    "w_scene_name, w_frame_index, w_show_tracks = (\n",
    "    helper_lib.create_interactive_widgets(EXAMPLE_SCENES, NUM_FRAMES,\n",
    "                                          ANNOTATIONS_DIR))\n",
    "\n",
    "# Some objects in the videos of CAD-Estate don't have a 3D shape aligned to\n",
    "# them, as automatic tracking (sec 3.1 of paper), object selection (sec. 3.2),\n",
    "# and pose estimation (sec. 3.5) can fail.\n",
    "# This visualization shows the 2D box tracks of such objects, when `show_tracks`\n",
    "# is true.\n",
    "\n",
    "\n",
    "@ipywidgets.interact\n",
    "def visualize_scene(scene_name: str = w_scene_name,\n",
    "                    frame_index: int = w_frame_index,\n",
    "                    show_tracks: bool = w_show_tracks):\n",
    "  ann_dir = fs.join(ANNOTATIONS_DIR, scene_name)\n",
    "  obj_json, frames_json = asyncio.run(\n",
    "      fs.read_all_bytes_async(\n",
    "          [fs.join(ann_dir, v) for v in [\"objects.json\", \"frames.json\"]]))\n",
    "  obj_json, frames_json = [json.loads(v) for v in (obj_json, frames_json)]\n",
    "  frames = frame_lib.load_metadata(frames_json)\n",
    "  objects = asyncio.run(obj_lib.load_objects(obj_json, shapenet_meta))\n",
    "\n",
    "  frames_dir = helper_lib.download_frames(frames, FRAMES_DIR)\n",
    "\n",
    "  if show_tracks:\n",
    "    num_frames = int(frames.manual_track_annotations.to(t.int32).sum())\n",
    "    if num_frames <= 0:\n",
    "      raise ValueError(\"The video has no manual track annotations.\")\n",
    "    frames = frame_lib.filter(frames, frames.manual_track_annotations)\n",
    "  else:\n",
    "    mask = frame_lib.sample_regular(frames, NUM_FRAMES)\n",
    "    frames = frame_lib.filter(frames, mask)\n",
    "    assert frames.frame_timestamps.shape[0] > 0\n",
    "\n",
    "  frames = asyncio.run(frame_lib.load_images(frames, frames_dir))\n",
    "\n",
    "  synth, rgb = helper_lib.render_objects(objects, frames, frame_index)\n",
    "\n",
    "  if show_tracks:\n",
    "    track_boxes = obj_lib.load_track_boxes(obj_json, frames)\n",
    "    synth = helper_lib.render_tracks(synth, objects, track_boxes, frame_index)\n",
    "\n",
    "  mask = (synth == 0).all(dim=0)\n",
    "  synth[:, mask] = rgb[:, mask]\n",
    "\n",
    "  _, sh, sw = synth.shape\n",
    "  h, w = sh * IMAGE_WIDTH // sw, IMAGE_WIDTH\n",
    "  synth = tvtF.resize(synth, (h, w), antialias=True)\n",
    "\n",
    "  disp(synth)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
